{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output\n",
    "from transformers import BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('../datasets/data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Four ways Bob Corker skewered Donald Trump  Image copyright Getty Images\\nOn Sunday morning, Donald Trump went off on a Twitter tirade against a member of his own party.\\nThis, in itself, isn\\'t exactly huge news. It\\'s far from the first time the president has turned his rhetorical cannons on his own ranks.\\nThis time, however, his attacks were particularly biting and personal. He essentially called Tennessee Senator Bob Corker, the chair of the powerful Senate Foreign Relations Committee, a coward for not running for re-election.\\nHe said Mr Corker \"begged\" for the president\\'s endorsement, which he refused to give. He wrongly claimed that Mr Corker\\'s support of the Iranian nuclear agreement was his only political accomplishment.\\nUnlike some of his colleagues, Mr Corker - free from having to worry about his immediate political future - didn\\'t hold his tongue.\\nSkip Twitter post by @SenBobCorker It\\'s a shame the White House has become an adult day care center. Someone obviously missed their shift this morning. â€” Senator Bob Corker (@SenBobCorker) October 8, 2017 Report\\nThat wasn\\'t the end of it, though. He then spoke with the New York Times and really let the president have it. Here are four choice quotes from the Tennessee senator\\'s interview with the Times and why they are particularly damning.\\n\"I don\\'t know why the president tweets out things that are not true. You know he does it, everyone knows he does it, but he does.\"\\nYou can\\'t really sugarcoat this one. Mr Corker is flat-out saying the president is a liar - and everyone knows it.\\nThe senator, in particular, is challenging Mr Trump\\'s insistence that he unsuccessfully pleaded for his endorsement, but the accusation is much broader.\\nMr Corker and the president used to be something akin to allies. The Tennessean was on Mr Trump\\'s short list for vice-president and secretary of state.\\nImage copyright Getty Images Image caption Bob Corker at Trump campaign rally in July 2016\\nThose days are seemingly very much over now - and it\\'s not like Mr Corker is going anywhere anytime soon. Although he\\'s not running for re-election, he\\'ll be in the Senate, chairing a powerful committee, until January 2019.\\nThe president\\'s margin for success in that chamber is razor-thin. If Democrats can continue to stand together in opposition, he can afford to lose only two votes out of 52 Republican senators. That\\'s why healthcare reform collapsed in July - and it could be bad news for tax efforts.\\nFrom here on out, Mr Corker isn\\'t going to do the president any favours.\\n\"Look, except for a few people, the vast majority of our caucus understands what we\\'re dealing with here.\"\\nFrustration in Congress has been growing over what Republicans feel has been the president\\'s inability to focus on advancing their agenda. Getting a sharply divided party to come together on plans to repeal Obamacare, reform taxes or boost infrastructure spending is challenging enough. Doing so when the president stirs up unrelated controversies on a seemingly daily basis makes things all the harder.\\nOne of the president\\'s gifts has been his ability to shake off negative stories by quickly moving on to a different subject. That worked brilliantly during his presidential campaign, but it\\'s less effective during the legislative slow grind.\\nImage copyright Getty Images Image caption Corker at the confirmation hearing for Secretary of State Rex Tillerson\\nFor months, Republicans in Congress have been grumbling about this in the background and among themselves. Occasionally, someone like Mr McConnell will lament that the president doesn\\'t understand how the Senate works.\\nMr Corker has now stated it loud and clear. And, what\\'s more, he says almost everyone agrees with him. They\\'ve kept silent until now because they still hope to pass conservative legislation that the president can sign or fear Mr Trump\\'s legions will back a primary challenge next year or stay home during the general election.\\nIf that calculus ever changes - if it becomes riskier to stay silent than speak out - Mr Trump will be in real trouble.\\n\"A lot of people think that there is some kind of \\'good cop, bad cop\\' act underway, but that\\'s just not true.\"\\nTime and again, Mr Trump has appeared to undercut Secretary of State Rex Tillerson and others in his administration who are attempting to use soft diplomacy to deal with a range of international crises.\\nThe war against the Taliban in Afghanistan, Iran\\'s compliance with the multinational nuclear agreement, the ongoing dispute between Qatar and its Persian Gulf neighbours, the unrest in Venezuela and, most recently, North Korea\\'s continued ballistic missile tests have all been the target of the president\\'s offhand remarks and Twitter invective.\\nSome administration defenders have said this is all a part of Mr Trump\\'s strategy - an updated version of the Nixon-era \"madman theory\", in which the president forces adversaries to give way because they fear an unpredictable US leader\\'s actions.\\nMr Corker isn\\'t buying it. There\\'s no strategy, he says, just the possibility of chaos - which he hopes Mr Trump\\'s senior advisers will be able to avoid.\\n\"I know for a fact that every single day at the White House, it\\'s a situation of trying to contain him.\"\\nThere\\'s now a growing collection of John Kelly face-palm photos that serve as a testament to the chief-of-staff\\'s reported frustration at dealing with the president.\\nMr Trump goes off-script to praise torch-bearing white nationalists at a rally in Charlottesville, and Mr Kelly is captured closing his eyes and rubbing the arch of his nose, as if attempting to stave off a migraine.\\nImage copyright Reuters Image caption White House Chief of Staff John Kelly looks on as US President Donald Trump speaks at a campaign rally\\nThe president calls North Korean leaders \"criminals\" in a speech to the United Nations, and Mr Kelly straight-up buries his face in his hands.\\nThe White House communications team is often left scrambling to try to explain or reframe an indelicate presidential \"joke\" or remark that directly contradicts what was until then the official administration line.\\nEven though Mr Kelly has brought some discipline to the West Wing staff, the president still marches to the beat of his own drum - and continues to have unfettered access to his phone\\'s Twitter app.\\nBob Corker is only the latest person - politician, journalist, sports star or celebrity - to feel the mercurial president\\'s uncontainable ire.'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_test = ['  '.join(x) for x in list(zip(data.Headline.values, data.Body.values))]\n",
    "to_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import fake_news_utils as fnu\n",
    "BATCH_SIZE = 4\n",
    "train_dataset = fnu.test_dataset_tokenized_sentences(to_test, data.Label.values)\n",
    "\n",
    "train_sampler = SequentialSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, tokens, masks=None):\n",
    "        pooled_output = self.bert(tokens, attention_mask=masks, output_hidden_states=False)[1]\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "# bert_clf = torch.load('/asd/CSML/Distributed Network Systems/fake-news-project/datasets/bert_clf.pth')\n",
    "bert_clf = BertBinaryClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "\r",
      "996/997.0 loss: 0.07762507525382689 \n"
     ]
    }
   ],
   "source": [
    "# Continue model training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)\n",
    "bert_clf = bert_clf.cuda()\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "param_optimizer = list(bert_clf.sigmoid.named_parameters())\n",
    "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = Adam(bert_clf.parameters(), lr=3e-6)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "\n",
    "        loss_func = nn.BCELoss()\n",
    "\n",
    "        batch_loss = loss_func(logits, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "\n",
    "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        clear_output(wait=True)\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(data.Label.values) / BATCH_SIZE, train_loss / (step_num + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/fake_news_cls-1')\n",
    "\n",
    "for step_num, batch_data in enumerate(train_dataloader):\n",
    "    writer.add_graph(bert_clf, tuple(batch_data)[:-1])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-24 16:46:53.444743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2022-05-24 16:46:53.469981: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\r\n",
      "2022-05-24 16:46:53.469998: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n",
      "Skipping registering GPU devices...\r\n",
      "\r\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\r\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\r\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\r\n",
      "\r\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\r\n",
      "TensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)\r\n",
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2120\n",
      "           1       0.99      0.99      0.99      1868\n",
      "\n",
      "    accuracy                           0.99      3988\n",
      "   macro avg       0.99      0.99      0.99      3988\n",
      "weighted avg       0.99      0.99      0.99      3988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "device = torch.device('cuda')\n",
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "\n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])\n",
    "\n",
    "np.mean(bert_predicted)\n",
    "\n",
    "print(classification_report(data.Label.values, bert_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(bert_clf, '../datasets/bert_clf-second.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}